{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "pl.Config.set_tbl_rows(50)\n",
    "import numpy as np\n",
    "import glob\n",
    "from gensim.models import Word2Vec\n",
    "from annoy import AnnoyIndex\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df_vector_MF(phase, method, ndim):\n",
    "    \"\"\"\n",
    "    行列分解から得られたuserとitemのembeddingをDataFrameに変換して保存する\n",
    "    \"\"\"\n",
    "    user_factor = np.load(f'input/embedding_{method}_{phase}/user_factors.npy')\n",
    "    user_factor = pl.concat([pl.DataFrame({'session':list(range(len(user_factor)))}), pl.DataFrame(user_factor, columns=[f'user_vec{i}' for i in range(ndim)])], how='horizontal')\n",
    "    user_factor = user_factor.with_column(np.sqrt(np.sum([pl.col(f'user_vec{i}')*pl.col(f'user_vec{i}') for i in range(ndim)])).alias('user_vec_norm'))\n",
    "    user_factor = user_factor.with_column(pl.col('session').cast(pl.Int32))\n",
    "    user_factor.write_parquet(f'user_vec_{method}_{phase}.parquet')\n",
    "    \n",
    "    item_factor = np.load(f'input/embedding_{method}_{phase}/item_factors.npy')    \n",
    "    item_factor = pl.concat([pl.DataFrame({'aid':list(range(len(item_factor)))}), pl.DataFrame(item_factor, columns=[f'item_vec{i}' for i in range(ndim)])], how='horizontal')\n",
    "    item_factor = item_factor.with_column(np.sqrt(np.sum([pl.col(f'item_vec{i}')*pl.col(f'item_vec{i}') for i in range(ndim)])).alias('item_vec_norm'))\n",
    "    item_factor = item_factor.with_column(pl.col('aid').cast(pl.Int32))\n",
    "    item_factor.write_parquet(f'item_vec_{method}_{phase}.parquet')\n",
    "        \n",
    "def make_df_top20_MF(phase, df_log, method, ndim):\n",
    "    \"\"\"\n",
    "    行列分解から得られたitemのembeddingから、各itemに対してcos類似度が高いitemを上位20個まで計算して保存する。\n",
    "    行列分解の計算にはuser-item_matrix_factorization.ipynbを用いる。\n",
    "    \"\"\"\n",
    "    item_factor = np.load(f'input/embedding_{method}_{phase}/item_factors.npy')\n",
    "    index = AnnoyIndex(ndim, 'angular')\n",
    "    for aid in range(len(item_factor)):\n",
    "        index.add_item(aid, item_factor[aid])    \n",
    "    index.build(100)\n",
    "    top20 = {aid:index.get_nns_by_item(aid, 21)[1:] for aid in df_log['aid'].unique()}\n",
    "    top20 = pl.DataFrame({'aid':list(top20.keys()), 'top_20':list(top20.values())}).explode('top_20')\n",
    "    top20 = top20.with_column(pl.col('*').cast(pl.Int32))\n",
    "    top20.write_parquet(f'top20_{method}_{phase}.parquet')\n",
    "\n",
    "def make_df_top20_w2vec(phase):\n",
    "    \"\"\"\n",
    "    word2vecから得られたitemのembeddingから、各itemに対してcos類似度が高いitemを上位20個まで計算して保存する。\n",
    "    word2vecの計算には以下の公開notebookを用いる。\n",
    "    https://www.kaggle.com/code/radek1/word2vec-how-to-training-and-submission\n",
    "    \"\"\"\n",
    "    index_w2vec = AnnoyIndex(50, 'angular')\n",
    "    w2vec = Word2Vec.load(f'input/word2vec_{phase}/word2vec.model')\n",
    "    aid2idx = {aid: i for i, aid in enumerate(w2vec.wv.index_to_key)}\n",
    "    for aid, idx in aid2idx.items():\n",
    "        index_w2vec.add_item(idx, w2vec.wv.vectors[idx])\n",
    "    index_w2vec.build(100)\n",
    "    top20_w2vec = {aid:[w2vec.wv.index_to_key[i] for i in index_w2vec.get_nns_by_item(aid2idx[aid], 21)[1:]] for aid in aid2idx.keys()}\n",
    "    top20_w2vec = pl.DataFrame({'aid':list(top20_w2vec.keys()), 'top_20':list(top20_w2vec.values())}).explode('top_20')\n",
    "    top20_w2vec = top20_w2vec.with_column(pl.col('*').cast(pl.Int32))\n",
    "    top20_w2vec.write_parquet(f'top20_w2vec_{phase}.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_log_recency_score(df, exp_min):\n",
    "    \"\"\"\n",
    "    userが最近click, add to cart, orderしたアイテムに対して大きい重みをつけるためのカラムを作成する\n",
    "    https://www.kaggle.com/code/radek1/polars-proof-of-concept-lgbm-rankerから引用\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.select([\n",
    "        pl.col('*'),\n",
    "        pl.col('session').cumcount().reverse().over('session').alias('action_num_reverse_chrono')\n",
    "    ])\n",
    "    \n",
    "    df = df.select([\n",
    "        pl.col('*'),\n",
    "        pl.col('session').count().over('session').alias('session_length')\n",
    "    ])\n",
    "    \n",
    "    linear_interpolation = exp_min + ((1-exp_min) / (df['session_length']-1)) * (df['session_length']-df['action_num_reverse_chrono']-1)\n",
    "    return df.with_columns([\n",
    "        pl.Series(2**linear_interpolation - 1).alias('log_recency_score').cast(pl.Float32)\n",
    "    ]).fill_nan(1).drop(['action_num_reverse_chrono', 'session_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pl.read_parquet('input/otto_train_and_test_data_for_local_validation/test.parquet')\n",
    "\n",
    "make_df_top20_MF('train', df_test, 'bpr', 101)\n",
    "make_df_top20_MF('train', df_test, 'lmf', 102)\n",
    "make_df_vector_MF('train', 'bpr', 101)\n",
    "make_df_vector_MF('train', 'lmf', 102)\n",
    "make_df_vector_MF('train', 'als', 50)\n",
    "make_df_top20_w2vec('train')\n",
    "\n",
    "top20_w2vec_train = pl.read_parquet('top20_w2vec_train.parquet')\n",
    "top20_bpr_train = pl.read_parquet('top20_bpr_train.parquet')\n",
    "user_vec_bpr_train = pl.read_parquet('user_vec_bpr_train.parquet')\n",
    "item_vec_bpr_train = pl.read_parquet('item_vec_bpr_train.parquet')\n",
    "top20_lmf_train = pl.read_parquet('top20_lmf_train.parquet')\n",
    "user_vec_lmf_train = pl.read_parquet('user_vec_lmf_train.parquet')\n",
    "item_vec_lmf_train = pl.read_parquet('item_vec_lmf_train.parquet')\n",
    "user_vec_als_train = pl.read_parquet('user_vec_als_train.parquet')\n",
    "item_vec_als_train = pl.read_parquet('item_vec_als_train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itemとitemのペアについて、同一のuserによってclick、add to cart, orderされた回数（共起回数）をカウントすることで得られる行列を\n",
    "# co-visitation matrixと呼ぶ。co-visitation matrixによって、例えばあるitemをclickしたuserはどのitemをclickする可能性が高いか知ることができる。\n",
    "# 以下の公開notebookを用いてco-visitation matrixを計算し、各itemについて共起回数が大きいitemを上位20個まで求めた。\n",
    "# https://www.kaggle.com/code/cdeotte/candidate-rerank-model-lb-0-575\n",
    "DISK_PIECES = 4\n",
    "VER = 6\n",
    "covisit_clicks_train = pl.concat([pl.read_parquet(f'input/covisitation_matrix_train/top_20_clicks_v{VER}_{i}.pqt') for i in range(DISK_PIECES)])\n",
    "covisit_carts_orders_train = pl.concat([pl.read_parquet(f'input/covisitation_matrix_train/top_20_carts_orders_v{VER}_{i}.pqt') for i in range(DISK_PIECES)])\n",
    "covisit_buy2buy_train = pl.read_parquet(f'input/covisitation_matrix_train/top_20_buy2buy_v{VER}_0.pqt')\n",
    "covisit_clicks_train = covisit_clicks_train.drop(['__index_level_0__']).rename({'aid_y':'aid'}).with_column(pl.col(['aid_x', 'aid']).cast(pl.Int32))\n",
    "covisit_carts_orders_train = covisit_carts_orders_train.drop(['__index_level_0__']).rename({'aid_y':'aid'}).with_column(pl.col(['aid_x', 'aid']).cast(pl.Int32))\n",
    "covisit_buy2buy_train = covisit_buy2buy_train.drop(['__index_level_0__']).rename({'aid_y':'aid'}).with_column(pl.col(['aid_x', 'aid']).cast(pl.Int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_item_similarity(phase, df, user_vector, item_vector, ndim, name):\n",
    "    \"\"\"\n",
    "    行列分解から得られたembeddingから、userとitemのcos類似度を計算して保存する\n",
    "    \"\"\"\n",
    "    cosine_similarity = np.sum([pl.col(f'item_vec{i}')*pl.col(f'user_vec{i}') for i in range(ndim)]) / pl.col('user_vec_norm') / pl.col('item_vec_norm')\n",
    "    session_chunks = map(list, np.array_split(df['session'].unique().to_list(), 10))\n",
    "    chunk_list = []\n",
    "    for chunk in session_chunks:\n",
    "        df_chunk = df[['session', 'aid']].filter(pl.col('session').is_in(chunk))\n",
    "        df_chunk = df_chunk.join(user_vector, on='session', how='left').join(item_vector, on='aid', how='left')\n",
    "        df_chunk = df_chunk.with_column(cosine_similarity.alias('user-item_similarity'))\n",
    "        chunk_list.append(df_chunk[['session', 'aid', 'user-item_similarity']])\n",
    "    pl.concat(chunk_list).write_parquet(f'user_item_similarity_{name}_{phase}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_candidate(phase, action_type, df, covisit_clicks, covisit_carts_orders, covisit_buy2buy,\n",
    "                   top20_w2vec, top20_bpr, top20_lmf, user_vector_bpr, item_vector_bpr, user_vector_lmf, item_vector_lmf,\n",
    "                  user_vector_als, item_vector_als):\n",
    "    \"\"\"\n",
    "    各userに対して、今後click, add to cart, orderすると考えられるitemの候補を選出する。\n",
    "    選出されるitemは、以下の通りである。\n",
    "    (1) userが過去にclick, add to cart, orderしたitem\n",
    "    (2) userが過去にclick, add to cart, orderしたitemとの共起回数が大きいitem\n",
    "    (3) userが過去にclick, add to cart, orderしたitemとのcos類似度が高いitem\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ndim_als = 50\n",
    "    ndim_lmf = 102\n",
    "    ndim_bpr = 101\n",
    "        \n",
    "    exp_min = {'click':0.05, 'cart':0.1, 'order':0.4}\n",
    "    df = add_log_recency_score(df, exp_min[action_type])\n",
    "\n",
    "    df_carts_orders = df.filter(pl.col('type').is_in([1, 2])).drop(['type', 'ts']).unique(subset=['session', 'aid'], keep='last')[::-1]\n",
    "    df = df.drop(['type', 'ts']).unique(subset=['session', 'aid'], keep='last')[::-1]\n",
    "    user_item_similarity(phase, df, user_vector_bpr, item_vector_bpr, 'userlog')\n",
    "    user_item_similarity(phase, df, user_vector_lmf, item_vector_lmf, ndim_lmf, 'userlog_lmf')\n",
    "    user_item_similarity(phase, df, user_vector_als, item_vector_als, ndim_als, 'userlog_als')\n",
    "\n",
    "    # userが過去に閲覧したitemとの共起回数が大きいitemを選出する\n",
    "    if action_type == 'click':\n",
    "        aids_covisit_clicks = df.rename({'aid':'aid_x'}).join(covisit_clicks, on='aid_x', how='left').drop_nulls(subset='aid')\n",
    "        aids_covisit_clicks = aids_covisit_clicks.groupby(['session', 'aid'], maintain_order=True).agg([pl.col('log_recency_score').sum().alias('co-visit'), (pl.col('wgt')*pl.col('log_recency_score')).sum()]).sort(['session', 'co-visit'], reverse=[False, True])\n",
    "        aids_covisit_clicks = aids_covisit_clicks.groupby('session', maintain_order=True).head(60)\n",
    "        aids_covisit_clicks.write_parquet(f'aids_covisit_click_{phase}.parquet')\n",
    "        user_item_similarity(phase, aids_covisit_clicks, user_vector_bpr, item_vector_bpr, 'covisit_click')\n",
    "        user_item_similarity(phase, aids_covisit_clicks, user_vector_lmf, item_vector_lmf, ndim_lmf, 'covisit_click_lmf')\n",
    "        user_item_similarity(phase, aids_covisit_clicks, user_vector_als, item_vector_als, ndim_als, 'covisit_click_als')\n",
    "    else:\n",
    "        aids_covisit_carts_orders = df.rename({'aid':'aid_x'}).join(covisit_carts_orders, on='aid_x', how='left').drop_nulls(subset='aid')\n",
    "        aids_covisit_buy2buy = df_carts_orders.rename({'aid':'aid_x'}).join(covisit_buy2buy, on='aid_x', how='left').drop_nulls(subset='aid')\n",
    "        aids_covisit_carts_orders = pl.concat([aids_covisit_carts_orders, aids_covisit_buy2buy])\n",
    "        aids_covisit_carts_orders = aids_covisit_carts_orders.groupby(['session', 'aid'], maintain_order=True).agg([pl.col('log_recency_score').sum().alias('co-visit'), pl.col('wgt').sum()]).sort(['session', 'co-visit'], reverse=[False, True])\n",
    "        aids_covisit_carts_orders = aids_covisit_carts_orders.groupby('session', maintain_order=True).head(80)\n",
    "        aids_covisit_carts_orders.write_parquet(f'aids_covisit_{action_type}_{phase}.parquet')\n",
    "        user_item_similarity(phase, aids_covisit_carts_orders, user_vector_bpr, item_vector_bpr, f'covisit_{action_type}')\n",
    "        user_item_similarity(phase, aids_covisit_carts_orders, user_vector_lmf, item_vector_lmf, ndim_lmf, f'covisit_{action_type}_lmf')\n",
    "        user_item_similarity(phase, aids_covisit_carts_orders, user_vector_als, item_vector_als, ndim_als, f'covisit_{action_type}_als')\n",
    "\n",
    "    # userが過去に閲覧したitemとのword2vecのembeddingのcos類似度が大きいitemを選出する\n",
    "    aids_w2vec = df.join(top20_w2vec, on='aid', how='left').groupby(['session', 'top_20'], maintain_order=True).agg(pl.col('log_recency_score').sum().alias('num_recommended_by_w2vec'))\n",
    "    aids_w2vec = aids_w2vec.sort(['session', 'num_recommended_by_w2vec'], reverse=[False, True]).rename({'top_20':'aid'})\n",
    "    aids_w2vec = aids_w2vec.groupby('session', maintain_order=True).head(20)\n",
    "    aids_w2vec.write_parquet(f'aids_w2vec_{action_type}_{phase}.parquet')\n",
    "    user_item_similarity(phase, aids_w2vec, user_vector_bpr, item_vector_bpr, f'w2vec_{action_type}')\n",
    "    user_item_similarity(phase, aids_w2vec, user_vector_lmf, item_vector_lmf, ndim_lmf, f'w2vec_{action_type}_lmf')\n",
    "    user_item_similarity(phase, aids_w2vec, user_vector_als, item_vector_als, ndim_als, f'w2vec_{action_type}_als')\n",
    "\n",
    "    # userが過去に閲覧したitemとのbpr行列分解のembeddingのcos類似度が大きいitemを選出する\n",
    "    aids_bpr = df.join(top20_bpr, on='aid', how='left').groupby(['session', 'top_20'], maintain_order=True).agg(pl.col('log_recency_score').sum().alias('num_recommended_by_bpr'))\n",
    "    aids_bpr = aids_bpr.sort(['session', 'num_recommended_by_bpr'], reverse=[False, True]).rename({'top_20':'aid'})\n",
    "    aids_bpr = aids_bpr.groupby('session', maintain_order=True).head(20)\n",
    "    aids_bpr.write_parquet(f'aids_bpr_{action_type}_{phase}.parquet')\n",
    "    user_item_similarity(phase, aids_bpr, user_vector_bpr, item_vector_bpr, f'bpr_{action_type}')\n",
    "    user_item_similarity(phase, aids_bpr, user_vector_lmf, item_vector_lmf, ndim_lmf, f'bpr_{action_type}_lmf')\n",
    "    user_item_similarity(phase, aids_bpr, user_vector_als, item_vector_als, ndim_als, f'bpr_{action_type}_als')\n",
    "    \n",
    "    # userが過去に閲覧したitemとのlogistic行列分解のembeddingのcos類似度が大きいitemを選出する\n",
    "    aids_lmf = df.join(top20_lmf, on='aid', how='left').groupby(['session', 'top_20'], maintain_order=True).agg(pl.col('log_recency_score').sum().alias('num_recommended_by_lmf'))\n",
    "    aids_lmf = aids_lmf.sort(['session', 'num_recommended_by_lmf'], reverse=[False, True]).rename({'top_20':'aid'})\n",
    "    aids_lmf = aids_lmf.groupby('session', maintain_order=True).head(20)\n",
    "    aids_lmf.write_parquet(f'aids_lmf_{action_type}_{phase}.parquet')  \n",
    "    user_item_similarity(phase, aids_lmf, user_vector_bpr, item_vector_bpr, ndim_bpr, f'lmf_{action_type}')\n",
    "    user_item_similarity(phase, aids_lmf, user_vector_lmf, item_vector_lmf, ndim_lmf, f'lmf_{action_type}_lmf')\n",
    "    user_item_similarity(phase, aids_lmf, user_vector_als, item_vector_als, ndim_als, f'lmf_{action_type}_als')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_candidate(phase, action_type, df):\n",
    "    \"\"\"\n",
    "    各基準によって選出したitemのDataFrameを結合し、1つのDataFrameにする\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.drop(['type', 'ts']).unique(subset=['session', 'aid'], keep='last')[::-1]\n",
    "\n",
    "    aids_covisit = pl.read_parquet(f'aids_covisit_{action_type}_{phase}.parquet')\n",
    "    aids_w2vec = pl.read_parquet(f'aids_w2vec_{action_type}_{phase}.parquet')\n",
    "    aids_bpr = pl.read_parquet(f'aids_bpr_{action_type}_{phase}.parquet')\n",
    "    aids_lmf = pl.read_parquet(f'aids_lmf_{action_type}_{phase}.parquet')\n",
    "\n",
    "    namelist = ['userlog', f'covisit_{action_type}', f'w2vec_{action_type}', f'bpr_{action_type}', f'lmf_{action_type}']\n",
    "    user2item_similarity = []\n",
    "    for name in namelist:\n",
    "        user2item_similarity.append(pl.read_parquet(f'user_item_similarity_{name}_{phase}.parquet'))\n",
    "    user2item_similarity = pl.concat(user2item_similarity).unique(subset=['session', 'aid']).rename({'user-item_similarity':'user-item_similarity_bpr'})\n",
    "    \n",
    "    user2item_similarity_lmf = []\n",
    "    for name in namelist:\n",
    "        user2item_similarity_lmf.append(pl.read_parquet(f'user_item_similarity_{name}_lmf_{phase}.parquet'))\n",
    "    user2item_similarity_lmf = pl.concat(user2item_similarity_lmf).unique(subset=['session', 'aid']).rename({'user-item_similarity':'user-item_similarity_lmf'})\n",
    "    \n",
    "    user2item_similarity_als = []\n",
    "    for name in namelist:\n",
    "        user2item_similarity_als.append(pl.read_parquet(f'user_item_similarity_{name}_als_{phase}.parquet'))\n",
    "    user2item_similarity_als = pl.concat(user2item_similarity_als).unique(subset=['session', 'aid']).rename({'user-item_similarity':'user-item_similarity_als'})\n",
    "        \n",
    "    df = df.join(aids_w2vec, on=['session', 'aid'], how='outer')\n",
    "    df = df.join(aids_bpr, on=['session', 'aid'], how='outer')\n",
    "    df = df.join(aids_lmf, on=['session', 'aid'], how='outer')\n",
    "    df = df.join(aids_covisit, on=['session', 'aid'], how='outer')\n",
    "    df = df.join(user2item_similarity, on=['session', 'aid'], how='left')\n",
    "    df = df.join(user2item_similarity_lmf, on=['session', 'aid'], how='left')\n",
    "    df = df.join(user2item_similarity_als, on=['session', 'aid'], how='left').sort('session')\n",
    "    \n",
    "    df.write_parquet(f'candidate_{action_type}_{phase}.parquet')\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "make_candidate('train', 'click', df_test, covisit_clicks_train, covisit_carts_orders_train, covisit_buy2buy_train,\n",
    "                top20_w2vec_train, top20_bpr_train, top20_lmf_train, user_vec_bpr_train, item_vec_bpr_train, user_vec_lmf_train, item_vec_lmf_train, user_vec_als_train, item_vec_als_train)\n",
    "make_candidate('train', 'cart', df_test, covisit_clicks_train, covisit_carts_orders_train, covisit_buy2buy_train,\n",
    "                top20_w2vec_train, top20_bpr_train, top20_lmf_train, user_vec_bpr_train, item_vec_bpr_train, user_vec_lmf_train, item_vec_lmf_train, user_vec_als_train, item_vec_als_train)\n",
    "make_candidate('train', 'order', df_test, covisit_clicks_train, covisit_carts_orders_train, covisit_buy2buy_train,\n",
    "                top20_w2vec_train, top20_bpr_train, top20_lmf_train, user_vec_bpr_train, item_vec_bpr_train, user_vec_lmf_train, item_vec_lmf_train, user_vec_als_train, item_vec_als_train)\n",
    "candidate_click_train = join_candidate('train', 'click', df_test)\n",
    "candidate_cart_train = join_candidate('train', 'cart', df_test)\n",
    "candidate_order_train = join_candidate('train', 'order', df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = pl.read_parquet('input/otto_train_and_test_data_for_local_validation/test_labels.parquet')\n",
    "train_label = train_label.explode('ground_truth').rename({'ground_truth':'aid'})\n",
    "train_label = train_label.with_column(pl.lit(1).alias('gt'))\n",
    "train_label = train_label.with_column(pl.col(['session', 'aid']).cast(pl.Int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "candidate_click_train = candidate_click_train.join(train_label.filter(pl.col('type')=='clicks')[['session', 'aid', 'gt']], on=['session', 'aid'], how='left')\n",
    "candidate_cart_train = candidate_cart_train.join(train_label.filter(pl.col('type')=='carts')[['session', 'aid', 'gt']], on=['session', 'aid'], how='left')\n",
    "candidate_order_train = candidate_order_train.join(train_label.filter(pl.col('type')=='orders')[['session', 'aid', 'gt']], on=['session', 'aid'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6044559661049003\n",
      "0.49199834026896594\n",
      "0.700609194926454\n"
     ]
    }
   ],
   "source": [
    "#全ての候補に対するrecall\n",
    "print(candidate_click_train['gt'].sum() / len(train_label.filter(pl.col('type')=='clicks')))\n",
    "print(candidate_cart_train['gt'].sum() / len(train_label.filter(pl.col('type')=='carts')))\n",
    "print(candidate_order_train['gt'].sum() / len(train_label.filter(pl.col('type')=='orders')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del candidate_click_train, candidate_cart_train, candidate_order_train, df_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pl.read_parquet('input/Otto_Full_Optimized_Memory_Footprint/test.parquet')\n",
    "\n",
    "make_df_top20_MF('test', df_test, 'bpr', 101)\n",
    "make_df_top20_MF('test', df_test, 'lmf', 102)\n",
    "make_df_vector_MF('test', 'bpr', 101)\n",
    "make_df_vector_MF('test', 'lmf', 102)\n",
    "make_df_vector_MF('test', 'als', 50)\n",
    "make_df_top20_w2vec('test')\n",
    "\n",
    "top20_bpr_test = pl.read_parquet('top20_bpr_test.parquet')\n",
    "user_vec_bpr_test = pl.read_parquet('user_vec_bpr_test.parquet')\n",
    "item_vec_bpr_test = pl.read_parquet('item_vec_bpr_test.parquet')\n",
    "top20_w2vec_test = pl.read_parquet('top20_w2vec_test.parquet')\n",
    "top20_lmf_test = pl.read_parquet('top20_lmf_test.parquet')\n",
    "user_vec_lmf_test = pl.read_parquet('user_vec_lmf_test.parquet')\n",
    "item_vec_lmf_test = pl.read_parquet('item_vec_lmf_test.parquet')\n",
    "user_vec_als_test = pl.read_parquet('user_vec_als_test.parquet')\n",
    "item_vec_als_test = pl.read_parquet('item_vec_als_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISK_PIECES = 4\n",
    "VER = 5\n",
    "covisit_clicks_test = pl.concat([pl.read_parquet(f'input/covisitation_matrix_test/top_20_clicks_v{VER}_{i}.pqt') for i in range(DISK_PIECES)])\n",
    "covisit_carts_orders_test = pl.concat([pl.read_parquet(f'input/covisitation_matrix_test/top_20_carts_orders_v{VER}_{i}.pqt') for i in range(DISK_PIECES)])\n",
    "covisit_buy2buy_test = pl.read_parquet(f'input/covisitation_matrix_test/top_20_buy2buy_v{VER}_0.pqt')\n",
    "covisit_clicks_test = covisit_clicks_test.drop(['__index_level_0__']).rename({'aid_y':'aid'}).with_column(pl.col(['aid_x', 'aid']).cast(pl.Int32))\n",
    "covisit_carts_orders_test = covisit_carts_orders_test.drop(['__index_level_0__']).rename({'aid_y':'aid'}).with_column(pl.col(['aid_x', 'aid']).cast(pl.Int32))\n",
    "covisit_buy2buy_test = covisit_buy2buy_test.drop(['__index_level_0__']).rename({'aid_y':'aid'}).with_column(pl.col(['aid_x', 'aid']).cast(pl.Int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "make_candidate('test', 'click', df_test, covisit_clicks_test, covisit_carts_orders_test, covisit_buy2buy_test,\n",
    "                top20_w2vec_test, top20_bpr_test, top20_lmf_test, user_vec_bpr_test, item_vec_bpr_test, user_vec_lmf_test, item_vec_lmf_test, user_vec_als_test, item_vec_als_test)\n",
    "make_candidate('test', 'cart', df_test, covisit_clicks_test, covisit_carts_orders_test, covisit_buy2buy_test,\n",
    "                top20_w2vec_test, top20_bpr_test, top20_lmf_test, user_vec_bpr_test, item_vec_bpr_test, user_vec_lmf_test, item_vec_lmf_test, user_vec_als_test, item_vec_als_test)\n",
    "make_candidate('test', 'order', df_test, covisit_clicks_test, covisit_carts_orders_test, covisit_buy2buy_test,\n",
    "                top20_w2vec_test, top20_bpr_test, top20_lmf_test, user_vec_bpr_test, item_vec_bpr_test, user_vec_lmf_test, item_vec_lmf_test, user_vec_als_test, item_vec_als_test)\n",
    "candidate_click_test = join_candidate('test', 'click', df_test)\n",
    "candidate_cart_test = join_candidate('test', 'cart', df_test)\n",
    "candidate_order_test = join_candidate('test', 'order', df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
